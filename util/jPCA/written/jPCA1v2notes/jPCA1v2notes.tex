\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\reals}{I\!\!R} %real numbers
\newcommand{\skewsym}{/\!\!\!\mathbb{S}}
\def\argmin{\qopname\relax n{argmin}}


\begin{document}

\title{jPCA1 vs. jPCA2}

\author{John P. Cunningham}


\maketitle


\section{Preliminaries}

We have been interested in matrices $M$, either unconstrained (that is, $M \in \reals^{n x n}$), or where $M$ is a skew-symmetric matrix, such that $M = -M^T$.  We say that such matrices $M_{skew} \in \skewsym^{n}$.  Matrices $M \in \reals^{n \times n}$ have $n^2$ variables, whereas matrices $M_{skew} \in \skewsym^{n}$ have only $\frac{n(n-1)}{2}$ variables (since the diagonal is 0 by definition, the lower triangle defines a matrix entirely).  Let's not worry about matrices and just think of these as vectors, where (using MATLAB notation) $m = M(:)$ and $m_{skew} = lowerTriangle(M_{skew})$.  By this definition, $m$ is a vector of length $n^2$ and $m_{skew}$ is a vector of length $\frac{n(n-1)}{2}$.  Ok great.
\\

Now let's suppose there is a matrix $H \in \{-1,0,1\}^{n \times \frac{n(n-1)}{2}}$, namely a matrix that maps $\frac{n(n-1)}{2}$ elements to $n^2$ elements.  The $\{-1,0,1\}$ part means that $H$ can only put values from an argument $v \in \reals^{\frac{n(n-1)}{2}}$ into the resulting $n^2$ vector (and perhaps flip the sign); $H$ can not do any weighted mixing.  Such a matrix exists and has some nice properties, but I won't bother you with them.
\\

This matrix $H$ is such that we can take $\frac{n(n-1)}{2}$ elements and turn that into $n^2$ elements, in particular we have $z = Hm_{skew}$.  This is the familiar operation of taking $\frac{n(n-1)}{2}$ elements and making a full skew-symmetric matrix out of them (just in vector form).  So too, if we take $y = H^Tm$, this is the familiar operation of taking a full matrix $M$ and getting the skew-symmetric part by doing $M-M^T$ (again, just in vector form instead of as matrices).  

One more thing.  In the stuff below, I'll use $d = dX(:)$ again to keep everything in vector notation.  $X$ then is a big block diagonal matrix with the original $X$ repeated on the diagonals. 

\section{jPCA1}

Ok so what.  Well, with this definition, here's what jPCA1 is actually doing.  Currently, we do (i)$M = X\backslash dX$, and then (ii)$M_{skew} = \frac{1}{2}(M - M^T)$.  With the notation above, what that is actually doing is:

\begin{equation}
\label{eqn:jpca1}
m_{skew} = \frac{1}{2}H^T(X^TX)^{-1}X^Td
\end{equation}

You should read this equation as ``(i) do least squares regression, and then (ii) get the skew-symmetric part."  Mathematically, it is:

\begin{eqnarray}
(i)~~~~~~~~~M^* & = & \argmin_{M \in \reals^{n \times n}} || dX - XM ||_F\\
(ii)~~~~~M_{skew} & = & \argmin_{K \in \skewsym^n} || M^* - K ||_F
\end{eqnarray}

These two steps are exactly the same as  (i)$M = X\backslash dX$, and then (ii)$M_{skew} = \frac{1}{2}(M - M^T)$.   Proving the first is just least squares, but proving the second is a bit tricky (interesting though).  I can give both proofs if you care, but regardless they are correct.


\section{jPCA2}

Now jPCA2 does directly what jPCA1 takes two (suboptimal) steps to do.   Specifically, it does:

\begin{equation}
\label{eqn:jpca2}
m_{skew} = (H^TX^TXH)^{-1}H^TX^Td
\end{equation}

Compare this equation (Eq \ref{eqn:jpca2}) with the above definition of jPCA1 in Equation \ref{eqn:jpca1}.  {\bf jPCA1 and jPCA2 are different, but they are both pretty simple (no matter what the code may look like). }  Importantly, the conditions for these two being the same are $X^TX = I$.  That happens with orthonormal columns in $X$.  By the way, with sample data just drawn as noise (as in a few of my examples), this is why jPCA1 and jPCA2 returned the same thing in the limit of infinite data.
\\

Finally, mathematically, here's what jPCA2 is doing:

\begin{eqnarray}
(i)~~~~~M_{skew} & = & \argmin_{M \in \skewsym^n} || dX - XM ||_F
\end{eqnarray}



\end{document}
