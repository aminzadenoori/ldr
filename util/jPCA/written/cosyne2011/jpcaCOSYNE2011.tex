\documentclass[12pt]{article}%[11pt,conference]{IEEEtran}
\usepackage{fullpage,times,cite,epsfig,textcomp,amsmath,amsfonts,color,rotating}

\usepackage[small, compact]{titlesec}
%\usepackage[small,compact]{titlesec}

%\usepackage[width=6.5in]{caption}

% Push out the margins
\oddsidemargin        0.50in
\evensidemargin       0.50in
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{-1in}
\topmargin    -59.0pt
\headheight   12pt
\headsep      0.15in
\columnsep         1.5pc
\textwidth        7.5in   % 2 x 21pc + 1pc = 43pc
\textheight 9.5in

% Add a header
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight 40pt
\rhead{JP Cunningham**, MM Churchland**,\\ MT Kaufman, KV Shenoy}
\lhead{Extracting rotational structure\\ from motor cortical data}
\cfoot{}

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0}
\renewcommand{\floatpagefraction}{0.95}



%%%%%%%%%%%%%%%%%%%%
\def\x{{\mathbf x}}
\def\y{{\mathbf y}}
\def\v{{\mathbf v}}
\def\u{{\mathbf u}}
\def\m{{\mathbf m}}
\def\k{{\mathbf k}}
\def\boldmu{\bm{\mu}}
\def\boldeta{\bm{\eta}}
\def\boldtheta{\bm{\theta}}
\def\regionA{\mathcal{A}}
\def\authorrefmark#1{$^{#1}$}

\def\vecx{\vec{x}}
\def\vecc{\vec{c}}
\def\veca{\vec{a}}
\def\vecp{\vec{p}}
\def\vecmu{\vec{\mu}}
\def\vecy{\vec{y}}
\def\sfP{\mathsf{P}}
\def\sfQ{\mathsf{Q}}
\def\sfV{\mathsf{V}}
\newcommand{\given}{\mid}

\newcommand{\reals}{I\!\!R} %real numbers
\newcommand{\skewsym}{/\!\!\!\mathbb{S}}
\def\argmin{\qopname\relax n{argmin}}

\newcommand{\todo}[1]{\textcolor{red}{[#1]}}
%%%%%%%%%%%%%%%%%%%%



\begin{document}
%\subsubsection*{Supplementary Figures}
%%%%%%%%%%%%%%%%%%%%
%\section*{Summary}
%
Dimensionality reduction techniques such as PCA are a cornerstone of analyzing high dimensional data.   PCA involves an eigenvalue decomposition of the data covariance matrix, which produces ranked orthogonal dimensions that can be used to linearly project the data to lower dimension.  The covariance matrix is but one choice of summary matrix that can be used for linear projections.   When the data are time series, the same decomposition can be used on a linear description of the dynamics (instead of the data covariance) to obtain a different projection.  This `dynamical PCA' produces projections representing the largest eigenvalues of the linear dynamical system.  Linear dynamical systems capture scaling and rotational aspects of the data (and combining scalings/rotations yields familiar features such as shear, projection, reflection, etc.).  Dynamical PCA makes no distinction between scaling and rotation, seeking only directions of largest and most consistent dynamical activity.  However, in some settings, one may hypothesize the existence of certain types of dynamics, and thus an algorithm is desired that can verify those dynamics.  For example, it is common for neural circuits to generate oscillations (or rotations), and we seek projections that best capture these fundamental aspects of the neural response.  Here we introduce jPCA, which specifically extracts projections of largest \emph{rotational} dynamics.   Using skew-symmetric matrices, the jPCA algorithm is an extension of fitting a linear dynamical system.  It has a unique, closed-form solution that can be quickly computed.  Importantly, like PCA and dynamical PCA, jPCA produces simple projection vectors - orthogonal linear directions - and so the interpretation of jPCA is identical to PCA or any other linear projection of the data.  We motivate and present details of the method, and we discuss its importance in extracting rotational structure from electrophysiological data recorded from primate motor cortex.
%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%
\section*{Additional Details of the jPCA algorithm}
%
We consider high dimensional time series data $\x(t) = [x_1(t),...,x_n(t)]$, which we write as a matrix $X \in \reals^{T \times n}$ ($T$ time points, $n$ data dimensions).  Fitting a time-invariant linear dynamical system (defined as $\dot\x(t) = \x(t)M$, where $M\in\reals^{n \times n}$) is simple least squares: we write $\dot X = XM$ and solve $M^* = \argmin_{M \in \reals^{n \times n}} || \dot X - XM ||_F$ (often written in closed form as $M^* =  (X^TX)^{-1}X^T\dot X$, or in MATLAB notation,  $M^* = X\backslash\dot X$).

Importantly, this dynamics matrix $M^*$ is a valid summary matrix describing the data, just like the data covariance $\Sigma = X^TX$ (and decomposition of $\Sigma$ results in PCA).  Just as with $\Sigma$, we can decompose $M^*$ to produce a valid set of linear projections: we call this approach `dynamical PCA.'  While PCA ignores temporal information and extracts only those directions in $n$ dimensional space that have the most variance, dynamical PCA extracts directions in the data that have consistent and strong linear dynamical properties.  

$M^*\in \reals^{n \times n}$ is the best general linear dynamical system describing $X$, without regard to scalings vs. rotations.   Here, instead, we are interested only in the \emph{rotational} aspects of the data.  Every linear transformation $M$ is some mixture of a symmetric matrix and a skew-symmetric matrix.  The skew-symmetric matrix (also called anti-symmetric, written symbolically as $\skewsym^{n \times n}$) is $M_{skew} = \frac{1}{2}(M - M^T)$.  Such matrices satisfy $M_{skew} = -M_{skew}^T$, have purely imaginary eigenvalues (in complex conjugate pairs), and describe only rotations in the data.  

Thus, whereas previously we solved $M^* = \argmin_{M \in \reals^{n \times n}} || \dot X - XM ||_F$, for rotational systems we solve $M_{skew}^* = \argmin_{M \in \skewsym^{n \times n}} || \dot X - XM ||_F$.   This problem has a unique optimum that can be quickly solved in closed form using a novel matrix-vector reparameterization that we introduce.   We call the algorithm solving this constrained problem (and then decomposing and projecting) jPCA.  Crucially, just like regular PCA and dynamical PCA, jPCA produces a ranked set of orthonormal vectors that we can use to project our data.  Again, the only difference is that, whereas in regular PCA we are interested in directions of highest variance, jPCA allows us to solve for highest frequency and consistency in \emph{rotational} linear dynamical systems.  

In working towards jPCA, we have developed three useful methods: dynamical PCA, jPCA, and symmetric PCA (the symmetric complement to jPCA).  These enable exploration of a variety of structured activity, not simply the highest variance (traditional PCA).  Finally, jPCA successfully projects the responses of tens to hundreds of neurons onto a plane that captures significant underlying rotational structure.  These results and their neuroscientific implications are discussed in Churchland**, Cunningham**, et al, COSYNE 2011.


%%%%%%%%%%%%%%%%%%%%


\paragraph*{Funding:} UK EPSRC EP/H019472/1, NIH Pioneer Award, Burroughs Wellcome, NIH CRCNS R01. 
%UK EPSRC EP/H019472/1, NIH Pioneer Award 1DP1OD006409, Burroughs Wellcome, NIH NINDS (CRCNS) R01-NS054283.

\bibliographystyle{IEEEbib.bst}
%\bibliographystyle{apalike}
\bibliography{jpca}



\end{document}


















