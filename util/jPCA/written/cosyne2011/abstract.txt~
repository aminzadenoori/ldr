By translating neural activity into useful behavioral commands for
assistive devices such as a prosthetic limb, neural prosthetic
systems seek to improve the lives of severely disabled
people. Despite exciting proofs of concept[1-3], the field has not
produced a system with adequate performance (speed and accuracy of the
device) to translate this technology to a clinically viable
device. There are many medical, scientific, and engineering challenges
in developing such a system[4-6], but all neural prosthetic systems must
interact in closed-loop with a human user.  This fact points to a
major obstacle in this important biomedical effort: the field does not
have an understanding of the closed-loop performance of its neural
decode algorithms.

Decode algorithms are typically applied offline to neural activity
previously gathered from a healthy animal (e.g., nonhuman primate),
and the decoded arm reach is then compared to the true movement that
corresponded to the recorded neural activity. However, this offline
testing almost certainly neglects important features of a real neural
prosthesis. Truly understanding decode performance requires the human
learning machine (the brain and motor plant) to be in closed-loop with
the decode algorithm. The field must investigate the extent to which
the subject can, for a given decode algorithm, engage feedback
mechanisms, learning and adaptation, and other control strategies to
improve decode performance. Closed-loop testing may suggest different
priorities for algorithmic development than offline analyses. However,
validating every algorithmic development in a closed-loop animal or
human clinical trial is infeasible given the huge cost and risk
associated with these studies. 

Here we ask the previously unaddressed research question: can we
exploit online, closed-loop prosthetic control strategies to validate
and prioritize algorithmic developments? We design and test a system
that allows healthy human subjects to use a prosthetic device driven
by synthetic neural activity, and we hypothesize that this online
human prosthetic simulator (OHPS) will inform the design of neural
prosthetic systems.

As a specific, testable prediction, we first test a key parameter
setting of the Kalman Filter, a popular algorithm for neural
prosthetic decoding.  Previous studies have found that decode error is
minimized when the time bin over which neural activity is integrated
(a windowed spike count) is around 280ms
(Wu et al (2006) Neural Comp). This bin width represents the time step
at which the algorithm updates its estimate of the decoded
reach. However, it may be that in a closed-loop experiment, this slow,
intermittent "hopping" behavior
of a decoded reach will frustrate the user. Perhaps better control
could be gained with a more frequent update.  It remains unclear how
this and other parameters should be set in future studies. Here we
address this question in the OHPS framework.

We have five healthy human subjects make real arm reaches to one of
eight distant targets (at 8cm from a central target) in a 3D
reaching environment (see Gilja, Nuyujukian, et al (2010)
COSYNE). Parameters of that real reach (recorded kinematics) are used
to generate synthetic neural activity via a standard Poisson spiking
process with underlying rate that is cosine tuned to the reach
velocity.  The simulated population has normally distributed preferred
directions and depths of modulation, and average firing rates are
chosen between 0 and 100 spikes per second.  We find that this
distribution of synthetic neurons produces decoded reaching behavior
similar to that of the same algorithm and task in our online
prosthetic experiments with non-human 

In control trials (about 100 per user), the user's true arm reach is rendered back to the
user in the virtual environment.  These control trials provide the
synthetic neural activity for the offline analysis (paralleling a
standard offline prosthesis experiment with non-human primates).  In
offline analysis, neural activity is integrated across a specified bin
width (50,100,150,200,250,300ms) and decoded using a Kalman filter into a reconstructed
offline decode. 

In online mode, each user then performs closed-loop, prosthetic decode reaches in
randomized blocks of 50 trials each.  Each block presents a real-time,
closed-loop prosthetic decode to the user, and the user is able to
bring to bear all of his/her behavioral modification strategies to
drive a successful reach.  The decodes within each block are generated
by a Kalman filter that is integrating neural activity (and updating
its decode position) at different time bins, selected from
{50,100,150,200,250,300}ms.  These online trials provide the
closed-loop performance metrics that we can measure to determine which
choices of this parameter optimize performance.

A number of metrics can be used to assess decode performance in this
online, closed-loop setting.  We have used several and have found
statistically significant reductions in error in all metrics.  In our
online data, we first tested failure rate (the proportion of
unsuccessful trials, where the user could not acquire the target) and
time to target (the amount of time required to reach the target).
Failure rate shows statistically significant reductions in failure
rate for bin widths of 100, 150, and 200ms (p<0.05).  Time to target shows
statistically significant reductions in failure rate for bin widths of
100 and 150 only (p<0.05).  In all cases, 50ms, 250ms, and 300ms performed
significantly more poorly than bin widths on the 100-200ms range.
These data are shown in supplementary figure 1.  Unfortunately, these
performance metrics are not sensible for offline decodes (offline failure rate
approaches 100% and time to target is infinite).  Thus, we also tested
a third performance metric - integrated distance to target - that can
be measured sensibly for offline and online decodes.  This metric
measures how quickly and accurately the decoded reach closes on the
target.  Again, we find a 
statistically significant performance optimum in the 100-150ms range
(p<0.05). This result is shown in supplementary figure 2a.  Importantly, this
first result indicates that the OHPS can be used to determine which
parameter settings are more or less controllable in an online system.
For the bin width of the Kalman filter, various metrics suggest the
performance optimum is between 100 and 200ms.

The second key contribution of this work is to show that online
analysis available from the OHPS suggests different parameter optima
than typical offline analysis.  Using our control trials, we performed
offline prosthetic decodes across the same set of Kalman filter bin
width parameters.  We fit both the offline data and the online data
(as above) to quadratic curves and recorded the minimum.  We used a
bootstrap (with 10000 samples) to find confidence intervals around the
estimates of the minima.  For online OHPS trials, we find a performance
optimum of 156ms (s.d. of 5.2ms).  For offline decode analysis, this
optimum is at XXXms (s.d. of XXms).  This is depicted in supplementary
figure 2b.  These minimum estimates demonstrate that there XXX a
significant difference between parameter optimizations done in offline
analysis and online, closed-loop analyses.  Since the eventual user
mode of these systems is online and closed-loop, the OHPS may be a
valuable tool for designing prosthetic decode algorithms and refining
parameter choices within particular algorithms.  We have shown that
here for the integration bin width of the Kalman filter.

Perhaps the gold standard for this parameter optimization
is an online, closed-loop real neural prosthetic system
in a non-human primate.  However, it is critical that we be able to
make accurate choices before going into this expensive and high-risk
animal model.  The OHPS enables this design engineering process.  We
designed and are using this system to inform our own closed-loop
prosthetic experiments with non-human primates (see Gilja, Nuyujukian,
et al (2010) COSYNE), so the results here feed directly (and will be
verified by) that experimental program. 

We have designed and tested a prosthetic simulator to investigate the
extent to which a prosthesis user can engage feedback mechanisms,
learning and adaptation, and other control strategies to improve
prosthetic performance. Closed-loop testing should suggest different
priorities for algorithmic development than offline analyses.  We have
taken one example, the integration window of a Kalman filter, and we
have shown: (1)that significant online performance differences are found at
different integration bin widths for a Kalman filter, suggesting an optimum of 100-200ms, and
(2)that closed-loop testing produces a different minimum than offline
analysis suggests, thereby indicating the important role of
closed-loop validation of algorithm and algorithmic parameter choices.
 
